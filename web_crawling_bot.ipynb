{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvvhUd+N0hvcnA7PewKMxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crux007/crux007/blob/main/web_crawling_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5_4DHOEJJkP8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_website(url):\n",
        "    # Send a GET request to the specified URL\n",
        "    response = requests.get(url)\n",
        "    \n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    \n",
        "    # Find all <a> tags (links) on the page\n",
        "    links = soup.find_all('a')\n",
        "    \n",
        "    # List to store the extracted titles\n",
        "    titles = []\n",
        "    \n",
        "    # Iterate over the links\n",
        "    for link in links:\n",
        "        # Get the href attribute of the link\n",
        "        href = link.get('href')\n",
        "        \n",
        "        # Make sure the href is not None and starts with 'http' (to avoid relative URLs)\n",
        "        if href and href.startswith('http'):\n",
        "            # Send a GET request to the linked page\n",
        "            linked_page = requests.get(href)\n",
        "            \n",
        "            # Parse the HTML content of the linked page\n",
        "            linked_soup = BeautifulSoup(linked_page.content, 'html.parser')\n",
        "            \n",
        "            # Get the title of the linked page\n",
        "            title = linked_soup.title.text if linked_soup.title else 'No title'\n",
        "            \n",
        "            # Add the title to the list\n",
        "            titles.append(title)\n",
        "            \n",
        "            # Recursive call to crawl the linked page\n",
        "            crawl_website(href)\n",
        "    \n",
        "    # Print the extracted titles\n",
        "    for title in titles:\n",
        "        print(title)"
      ],
      "metadata": {
        "id": "-Q5XFJuoLcIZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawl_website('https://utupay.co')"
      ],
      "metadata": {
        "id": "0Nb0NkktLibn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the web crawler in the provided code is to visit web pages, extract the titles of those pages, and print them out. The crawler starts from a given URL and recursively follows the links it finds on each page, visiting and parsing the HTML content of those linked pages as well.\n",
        "\n",
        "By extracting and printing the titles of the web pages, this code provides a basic demonstration of how a web crawler can retrieve specific information from a website. However, the actual purpose and evaluation of a web crawler depend on the specific requirements and goals of the project or application you are developing.\n",
        "\n",
        "Here are a few examples of purposes and evaluation criteria for a web crawler:\n",
        "\n",
        "Data Collection: The crawler can be used to collect data from websites, such as scraping product information, news articles, or user reviews. The evaluation would involve assessing the accuracy and completeness of the collected data, as well as the efficiency and reliability of the crawling process.\n",
        "\n",
        "SEO Analysis: The crawler can be employed to analyze a website's search engine optimization (SEO) by examining factors like page titles, meta tags, and internal links. The evaluation might involve assessing the crawler's ability to identify and analyze these elements accurately.\n",
        "\n",
        "Link Analysis: The crawler can be used to analyze the structure of a website by following and evaluating links. This analysis can help identify broken links, orphan pages, or potential areas for optimization. Evaluation criteria might include the ability to discover and analyze various types of links accurately.\n",
        "\n",
        "Content Monitoring: The crawler can be employed to monitor changes to specific web pages or websites over time. Evaluation could involve measuring the crawler's ability to detect and report changes accurately and in a timely manner.\n",
        "\n",
        "When evaluating a web crawler, consider factors like performance (speed and efficiency), accuracy of data extraction, handling of different website structures, adherence to ethical guidelines and terms of service, handling of dynamic content (JavaScript-driven websites), and any specific requirements related to the project or application you are working on."
      ],
      "metadata": {
        "id": "09yynEnzM7dA"
      }
    }
  ]
}